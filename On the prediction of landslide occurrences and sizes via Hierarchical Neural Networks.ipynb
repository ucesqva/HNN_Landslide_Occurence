{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Dropout\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.regularizers import l2\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.optimizers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from random import seed\n",
    "from random import choice\n",
    "from tqdm import tqdm\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Function Library</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the function used to build the two output structure of the models\n",
    "\n",
    "def build_multi_output_model(input_size, stage_1_hidden_layers, stage_2_hidden_layers, \n",
    "                            stage_1_neurons, stage_2_neurons, dropout_rate, l2):\n",
    "    input_size=input_size\n",
    "    stage1_counter = 0\n",
    "    stage2_counter = 0\n",
    "    input_layer = keras.layers.Input(input_size, \n",
    "                                     name=\"input\")\n",
    "    while stage1_counter < (stage_1_hidden_layers):\n",
    "        if stage1_counter == 0:\n",
    "            x = keras.layers.Dense(stage_1_neurons, activation=\"relu\", \n",
    "                                   kernel_regularizer=regularizers.l2(l2)\n",
    "                                  )(input_layer)\n",
    "            x = keras.layers.Dropout(dropout_rate)(x)\n",
    "            stage1_counter += 1\n",
    "        else:\n",
    "            x = keras.layers.Dense(stage_1_neurons, activation=\"relu\",\n",
    "                                  kernel_regularizer=regularizers.l2(l2)\n",
    "                                  )(x)\n",
    "            x = keras.layers.Dropout(dropout_rate)(x)\n",
    "            stage1_counter += 1    \n",
    "    count_output = keras.layers.Dense(1, activation=\"sigmoid\",\n",
    "                                      name=\"count_output\")(x)\n",
    "    while stage2_counter < (stage_2_hidden_layers):\n",
    "        if stage1_counter == 0:\n",
    "            x = keras.layers.Dense(stage_2_neurons, activation=tf.keras.layers.LeakyReLU(alpha=0.1), \n",
    "                                   kernel_regularizer=regularizers.l2(l2)\n",
    "                                  )(count_output)\n",
    "            x = keras.layers.Dropout(dropout_rate)(x)\n",
    "            stage2_counter += 1\n",
    "        else:\n",
    "            x = keras.layers.Dense(stage_2_neurons, activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "                                  kernel_regularizer=regularizers.l2(l2)\n",
    "                                  )(x)\n",
    "            x = keras.layers.Dropout(dropout_rate)(x)\n",
    "            stage2_counter += 1\n",
    "    class_output = keras.layers.Dense(4, activation=\"softmax\",\n",
    "                                      name=\"class_output\")(x)\n",
    "    model = keras.Model(inputs=input_layer, outputs=[count_output, class_output]) \n",
    "    return model\n",
    "\n",
    "\n",
    "def compile_model(model, parameters, bin_weight, cla_weight):\n",
    "    if optimal_parameters[-1] == 'adam':\n",
    "        opt = 'adam'\n",
    "    else:\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate = optimal_parameters[-1])\n",
    "        \n",
    "    model.compile(loss=[\"binary_crossentropy\", \"categorical_crossentropy\"], \n",
    "              optimizer=opt, loss_weights=[bin_weight, cla_weight])\n",
    "\n",
    "\n",
    "# Generate a random configuration of hyperparameters from the search space\n",
    "def generate_configuration(first_stage_hl, second_stage_hl, first_stage_n, second_stage_n, \n",
    "                           dropout_rate, l2, learning_rate):\n",
    "    c1 = choice(first_stage_hl)\n",
    "    c2 = choice(second_stage_hl)\n",
    "    c3 = choice(first_stage_n)\n",
    "    c4 = choice(second_stage_n)\n",
    "    c5 = choice(dropout_rate)\n",
    "    c6 = choice(l2)\n",
    "    c7 = choice(learning_rate)\n",
    "\n",
    "    configuration = [19, c1, c2, c3, c4, c5, c6, c7] # 19 is the number of explanatory variables\n",
    "    return configuration\n",
    "\n",
    "\n",
    "# MC tuning process\n",
    "def monte_carlo_tuning(x_train, y_train_1, y_train_2,\n",
    "                       runs, n_samples):\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                          mode='min', verbose=0, patience=0, restore_best_weights=True)\n",
    "    config_cache_loss = {}\n",
    "    config_cache_count = {}\n",
    "    config_cache_area = {}\n",
    "    for _ in tqdm(range(runs)):\n",
    "        configuration = generate_configuration(first_stage_hl, second_stage_hl, first_stage_n, second_stage_n, \n",
    "                           dropout_rate, l2, learning_rate)\n",
    "        \n",
    "        if all(configuration) not in config_cache_loss:\n",
    "            sample_loss_cache = []\n",
    "            for i in range(n_samples):\n",
    "                model = build_multi_output_model(configuration[0], configuration[1],\n",
    "                                               configuration[2], configuration[3], \n",
    "                                                configuration[4], configuration[5], configuration[6])\n",
    "                \n",
    "                if configuration[7] == 'adam':\n",
    "                    opt = 'adam'\n",
    "                else:\n",
    "                    opt = tf.keras.optimizers.Adam(\n",
    "                                    learning_rate = configuration[7])\n",
    "                    \n",
    "                model.compile(loss=[\"binary_crossentropy\", \"categorical_crossentropy\"], \n",
    "                                                optimizer=opt,\n",
    "                                                loss_weights=[0.5, 0.5])\n",
    "                history = model.fit(x_train, [y_train_1, y_train_2], \n",
    "                    epochs=1, #update epochs\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=2),\n",
    "                    verbose=0)   \n",
    "                sample_loss = history.history['val_loss'][-1]\n",
    "                sample_loss_cache.append(sample_loss)\n",
    "            loss = sum(sample_loss_cache)/n_samples\n",
    "            config_cache_loss[str(configuration)]  = loss\n",
    "    return config_cache_loss\n",
    "\n",
    "\n",
    "# Given a datalist and a number of breakpoints, this function finds the break point values such that the\n",
    "# variance in each category is minimised. This code is inspired by: https://gist.github.com/drewda/1299198\n",
    "# The code at the link provided does not work properly, however changes have been made here to fix it. \n",
    "def getJenksBreaks(dataList, numClass):\n",
    "    dataList = list(dataList)\n",
    "    dataList.sort()\n",
    "    mat1 = []\n",
    "    for i in range(0,len(dataList)+1):\n",
    "        temp = []\n",
    "        for j in range(0,numClass+1):\n",
    "            temp.append(0)\n",
    "        mat1.append(temp)\n",
    "    mat2 = []\n",
    "    for i in range(0,len(dataList)+1):\n",
    "        temp = []\n",
    "        for j in range(0,numClass+1):\n",
    "            temp.append(0)\n",
    "        mat2.append(temp)\n",
    "    for i in range(1,numClass+1):\n",
    "        mat1[1][i] = 1\n",
    "        mat2[1][i] = 0\n",
    "        for j in range(2,len(dataList)+1):\n",
    "            mat2[j][i] = float('inf')\n",
    "    v = 0.0\n",
    "    for l in range(2,len(dataList)+1):\n",
    "        s1 = 0.0\n",
    "        s2 = 0.0\n",
    "        w = 0.0\n",
    "        for m in range(1,l+1):\n",
    "            i3 = l - m + 1\n",
    "            val = float(dataList[i3-1])\n",
    "            s2 += val * val\n",
    "            s1 += val\n",
    "            w += 1\n",
    "            v = s2 - (s1 * s1) / w\n",
    "            i4 = i3 - 1\n",
    "            if i4 != 0:\n",
    "                for j in range(2,numClass+1):\n",
    "                    if mat2[l][j] >= (v + mat2[i4][j - 1]):\n",
    "                        mat1[l][j] = i3\n",
    "                        mat2[l][j] = v + mat2[i4][j - 1]\n",
    "        mat1[l][1] = 1\n",
    "        mat2[l][1] = v\n",
    "    k = len(dataList)\n",
    "    breaks = []\n",
    "    for i in range(0,numClass+1):\n",
    "        breaks.append(min(dataList))\n",
    "    countNum = numClass\n",
    "\n",
    "    while countNum >= 2:#print \"rank = \" + str(mat1[k][countNum])\n",
    "        id = int((mat1[k][countNum]) - 2)\n",
    "        value = dataList[id]\n",
    "        breaks.append(value)\n",
    "        k = int((mat1[k][countNum] - 1))\n",
    "        countNum -= 1\n",
    "    breaks.append(max(dataList))\n",
    "    breaks = list(set(breaks))\n",
    "    breaks.sort()\n",
    "    return(breaks)\n",
    "    # The first output number is the smallest value in the input\n",
    "    # Following output numbers are inclusive upper bounds\n",
    "\n",
    "\n",
    "# The next two functions are used to determine the goodness of variance fit, determining how well the function above\n",
    "# performed (in other words, it measures how well using a specific number of breakpoints applies to the data)\n",
    "# This code is informed by a stack exchange comment availible at this link:\n",
    "# https://stats.stackexchange.com/questions/143974/jenks-natural-breaks-in-python-how-to-find-the-optimum-number-of-breaks\n",
    "\n",
    "def goodness_of_variance_fit(array, nclasses):\n",
    "    \n",
    "        classes = getJenksBreaks(array, nclasses)\n",
    "        classified = np.array([classify(i, classes) for i in array])\n",
    "        maxz = max(classified)\n",
    "        zone_indices = [[idx for idx, val in enumerate(classified) if zone + 1 == val] for zone in range(maxz)]\n",
    "        sdam = np.sum((array - array.mean()) ** 2)\n",
    "        array_sort = [np.array([array[index] for index in zone]) for zone in zone_indices]\n",
    "        sdcm = sum([np.sum((classified - classified.mean()) ** 2) for classified in array_sort])\n",
    "        gvf = (sdam - sdcm) / sdam\n",
    "        return gvf, classes\n",
    "    \n",
    "def classify(value, breaks):\n",
    "    for i in range(1, len(breaks)):\n",
    "        if value < breaks[i]:\n",
    "            return i\n",
    "    return len(breaks) - 1\n",
    "\n",
    "\n",
    "# This function iterates over various numbers of breakpoints and uses the function above to find the best \n",
    "# number of break points\n",
    "\n",
    "def optimal_classes(data, gvf_threshold, min_classes):\n",
    "\n",
    "    gvf= 0.0\n",
    "    nclasses = min_classes\n",
    "\n",
    "    while gvf < gvf_threshold:\n",
    "            gvf, classes = goodness_of_variance_fit(data, nclasses)\n",
    "            if gvf < gvf_threshold:\n",
    "                gvf, classes = goodness_of_variance_fit(data, nclasses)\n",
    "                print('Classes Trialed: ', nclasses)\n",
    "                print('gvf value :', gvf)\n",
    "                print('Outcome: gvf value insufficient, further trials will be attempted')\n",
    "                print()\n",
    "                nclasses += 1\n",
    "            else:\n",
    "                print('Classes Trialed: ', nclasses)\n",
    "                print('gvf value: ', gvf)\n",
    "                print('Outcome: gvf value broke threshold, breaks found.')\n",
    "                print('Breaks: ', classes)\n",
    "    return classes\n",
    "                \n",
    "\n",
    "def plot_multiclass_roc(y_test, y_pred, n_classes, figsize=(17, 6)):\n",
    "\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    y_test_dummies = pd.get_dummies(y_test, drop_first=False).values\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test_dummies[:, i], y_pred[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.plot([0, 1], [0, 1], 'k--')\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title('ROC Curves for Each Class')\n",
    "    for i in range(n_classes):\n",
    "        ax.plot(fpr[i], tpr[i], label='ROC curve (area = %0.6f) for label %i' % (roc_auc[i], i))\n",
    "    ax.legend(loc=\"best\")\n",
    "    ax.grid(alpha=.4)\n",
    "#     plt.savefig(\"class_model_ROC.pdf\") <- Uncomment to save graph\n",
    "    plt.show()\n",
    "    \n",
    "    return roc_auc\n",
    "    \n",
    "# Test different different weight combinations \n",
    "def loss_weight_ratio_testing(input_model, ratio_increase):\n",
    "    weight_1 = 0\n",
    "    weight_2 = 10\n",
    "    count_roc =[]\n",
    "    class_ovr_w_roc =[]\n",
    "    model_histories = []\n",
    "    weight_ratio = []\n",
    "    \n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25)\n",
    "    \n",
    "    while weight_1 <= 10:\n",
    "        model = input_model\n",
    "        \n",
    "        model.compile(loss=[\"binary_crossentropy\", \"categorical_crossentropy\"], \n",
    "              optimizer='adam',\n",
    "              loss_weights=[weight_1, weight_2])\n",
    "\n",
    "        history = model.fit(x_train, [y_train_count, y_train_area], \n",
    "                    epochs=15,#30\n",
    "                    verbose = 0,\n",
    "                    validation_split=0.2\n",
    "                   )\n",
    "        \n",
    "        y_pred = model.predict(x_test)\n",
    "        count_predictions, area_predictions = y_pred[0], y_pred[1]\n",
    "        \n",
    "        c_auc = roc_auc_score(y_test_count, count_predictions)\n",
    "        \n",
    "        w_ovr = roc_auc_score(y_test_area, area_predictions, multi_class=\"ovo\",\n",
    "                                     average=\"weighted\")\n",
    "\n",
    "        # summarize scores\n",
    "        print('Count Weight: ', weight_1, 'Class Weight: ', weight_2)\n",
    "        print('Count ROC AUC=%.6f' % (c_auc))\n",
    "        print('Class Weighted OVR ROC:', w_ovr)\n",
    "        print()                  \n",
    "        count_roc.append(c_auc)\n",
    "        class_ovr_w_roc.append(w_ovr)\n",
    "        model_histories.append(history)\n",
    "        wr = 0\n",
    "        if weight_2 == 0:\n",
    "            weight_ratio.append(wr)\n",
    "        else:\n",
    "            wr = (weight_1/weight_2)\n",
    "            weight_ratio.append(wr)\n",
    "        weight_1 += ratio_increase\n",
    "        weight_2 = 10-weight_1\n",
    "    return count_roc, class_ovr_w_roc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Load & Preprocess Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"landslide_data.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step is to use Fischer-Jenks to get the area classes\n",
    "\n",
    "# Create a df of non-zero Area_Slide\n",
    "filtered_data = data[data['Area_Slide'] > 0]\n",
    "non_zero_slide_area = filtered_data['Area_Slide']\n",
    "non_zero_slide_area = np.array(non_zero_slide_area)\n",
    "\n",
    "# Find the optimal breaks in non-zero Area_Slide, gvf threshold of 0.85, start by trying two classes\n",
    "optimal_classes = optimal_classes(non_zero_slide_area, 0.85, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a class value to each record based on the above results\n",
    "data.loc[(data[\"Area_Slide\"] == 0) & (data[\"Area_Slide\"] < optimal_classes[0]), \"Slide_Class\"] = \"Class 0\" \n",
    "data.loc[(data[\"Area_Slide\"] >= optimal_classes[0]) & (data[\"Area_Slide\"] <= optimal_classes[1]), \"Slide_Class\"] = \"Class 1\" \n",
    "data.loc[(data[\"Area_Slide\"] > optimal_classes[1]) & (data[\"Area_Slide\"] <= optimal_classes[2]), \"Slide_Class\"] = \"Class 2\" \n",
    "data.loc[(data[\"Area_Slide\"] > optimal_classes[2]) & (data[\"Area_Slide\"] <= optimal_classes[3]), \"Slide_Class\"] = \"Class 3\" \n",
    "data.loc[(data[\"Area_Slide\"] > optimal_classes[3]) & (data[\"Area_Slide\"] <= optimal_classes[4]), \"Slide_Class\"] = \"Class 4\"\n",
    "data.loc[(data[\"Area_Slide\"] > optimal_classes[4]) & (data[\"Area_Slide\"] <= optimal_classes[5]), \"Slide_Class\"] = \"Class 5\"\n",
    "data[\"Slide_Class\"].value_counts()\n",
    "\n",
    "# Observe: 1) The imbalanced distribution, and 2) Class 5 only has one record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge select classes, rationale in paper\n",
    "data.loc[(data[\"Area_Slide\"] == 0) & (data[\"Area_Slide\"] < optimal_classes[0]), \"Slide_Class\"] = \"Class 0\" \n",
    "data.loc[(data[\"Area_Slide\"] >= optimal_classes[0]) & (data[\"Area_Slide\"] <= optimal_classes[2]), \"Slide_Class\"] = \"Class 1\"\n",
    "data.loc[(data[\"Area_Slide\"] > optimal_classes[2]) & (data[\"Area_Slide\"] <= optimal_classes[3]), \"Slide_Class\"] = \"Class 2\" \n",
    "data.loc[(data[\"Area_Slide\"] > optimal_classes[3]) & (data[\"Area_Slide\"] <= optimal_classes[5]), \"Slide_Class\"] = \"Class 3\"\n",
    "data[\"Slide_Class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Uncomment this cell to bypass the process of the three preceeding cells ###\n",
    "data.loc[(data[\"Area_Slide\"] == 0) & (data[\"Area_Slide\"] < 0.054691), \"Slide_Class\"] = \"Class 0\" \n",
    "data.loc[(data[\"Area_Slide\"] >= 0.054691) & (data[\"Area_Slide\"] <= 2981.98645), \"Slide_Class\"] = \"Class 1\" \n",
    "data.loc[(data[\"Area_Slide\"] > 2981.98645) & (data[\"Area_Slide\"] <= 10821.34832), \"Slide_Class\"] = \"Class 1\" \n",
    "data.loc[(data[\"Area_Slide\"] > 10821.34832) & (data[\"Area_Slide\"] <= 29509.81466), \"Slide_Class\"] = \"Class 2\" \n",
    "data.loc[(data[\"Area_Slide\"] > 29509.81466) & (data[\"Area_Slide\"] <= 82850.15392), \"Slide_Class\"] = \"Class 3\"\n",
    "data.loc[(data[\"Area_Slide\"] > 82850.15392) & (data[\"Area_Slide\"] <= 182966.8526), \"Slide_Class\"] = \"Class 3\"\n",
    "data[\"Slide_Class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dependant and explanatory variables\n",
    "data_y = data['Slide_Class']\n",
    "data_x = data.drop(['SU_ID', 'Count', 'Area_Slide', 'Slide_Class'], axis=1)\n",
    "data_x = data_x.astype(float)\n",
    "\n",
    "\n",
    "# Train/Test split\n",
    "x_train, x_test, y_train_area, y_test_area = train_test_split(data_x, data_y, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "# Standardize the explanatory variables\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "train_x_values = x_train.values\n",
    "x_scaled1 = scaler.fit_transform(train_x_values)\n",
    "x_train = pd.DataFrame(x_scaled1)\n",
    "\n",
    "test_x_values = x_test.values\n",
    "x_scaled2 = scaler.fit_transform(test_x_values)\n",
    "x_test = pd.DataFrame(x_scaled2)\n",
    "\n",
    "\n",
    "# Use SMOTE package to oversample training data\n",
    "oversample = SMOTE()\n",
    "x_train, y_train_area = oversample.fit_resample(x_train, y_train_area)\n",
    "\n",
    "\n",
    "# Use the Slide_Class field to create a binary variable that \n",
    "# indicates whether or not there is a landslide in any capacity\n",
    "y_train_count = []\n",
    "y_test_count = []\n",
    "for i in y_train_area:\n",
    "    if i == \"Class 0\":\n",
    "        y_train_count.append(0)\n",
    "    else:\n",
    "        y_train_count.append(1)\n",
    "        \n",
    "for i in y_test_area:\n",
    "    if i == \"Class 0\":\n",
    "        y_test_count.append(0)\n",
    "    else:\n",
    "        y_test_count.append(1)     \n",
    "y_train_count = np.array(y_train_count)\n",
    "y_test_count = np.array(y_test_count)\n",
    "\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# encode training\n",
    "encoder.fit(y_train_area)\n",
    "encoded_Y = encoder.transform(y_train_area)\n",
    "y_train_area = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "# encode testing\n",
    "# encoder.fit(y_test_area)\n",
    "# encoded_Y_test = encoder.transform(y_test_area)\n",
    "# y_test_area = np_utils.to_categorical(encoded_Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Modelling</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the search space for MC tuning\n",
    "first_stage_hl  = [6, 12, 18, 24, 48]\n",
    "second_stage_hl = [6, 12, 18, 24]\n",
    "first_stage_n   = [4, 8, 16, 32, 64]\n",
    "second_stage_n  = [4, 8, 16, 32, 64]\n",
    "dropout_rate    = [0.2, 0.4, 0.6, 0.8]\n",
    "l2              = [1e-3, 1e-4, 1e-5]\n",
    "learning_rate   = [1e-2, 1e-3, 1e-4, \"adam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE MONTE CARLO TUNING FUNCTION \n",
    "\n",
    "# Uncomment the code below to run the montecarlo tuner. Be advised, this process is time consuming and you may \n",
    "# prefer to simply reuse the values in the report. \n",
    "\n",
    "# config_cache = monte_carlo_tuning(x_train, y_train_count, y_train_area, runs=1000, n_samples=5)\n",
    "# optimal_parameters_string = min(config_cache, key=config_cache.get)\n",
    "# optimal_parameters = ast.literal_eval(optimal_parameters_string)\n",
    "\n",
    "optimal_parameters = [19, 6, 12, 32, 4, 0.2, 1e-4, 'adam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lwr_model = build_multi_output_model(*optimal_parameters[:-1]) # Leave out the learning rate for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is also a time consuming process, uncomment the vaules in the next cell to reuse the results from the report\n",
    "co, cl = loss_weight_ratio_testing(lwr_model, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = list(range(1,12))\n",
    "# co = [0.4697642202160677,\n",
    "#   0.8732161398821352,\n",
    "#   0.8738895262157222,\n",
    "#   0.872348897639263,\n",
    "#   0.8735841626784037,\n",
    "#   0.8747538702127408,\n",
    "#   0.8755926742685977,\n",
    "#   0.8767841662705956,\n",
    "#   0.8773556105481453,\n",
    "#   0.8789159417536946,\n",
    "#   0.8822436314449064]\n",
    "# cl = [0.8670034294002861,\n",
    "#   0.8719836649189152,\n",
    "#   0.8721181496411214,\n",
    "#   0.8706711380269829,\n",
    "#   0.8717760611632063,\n",
    "#   0.8731124107537576,\n",
    "#   0.8740722810400376,\n",
    "#   0.8746425163405755,\n",
    "#   0.8752726948902124,\n",
    "#   0.8747853344452116,\n",
    "#   0.5008439865358839]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "plt.plot(iteration, cl, 'o-', color='blue')\n",
    "plt.plot(iteration, co, 'o-', color='red')\n",
    "\n",
    "plt.title('Loss Weight Ratio Testing ROC AUC Results')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('ROC AUC Value (One Vs. Rest Weighted for Class Output)')\n",
    "    \n",
    "red_patch = mpatches.Patch(color='red', label='Binary Output')\n",
    "blu_patch = mpatches.Patch(color='blue', label='Class Output')\n",
    "plt.legend(handles=[red_patch, blu_patch])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "plt.plot(iteration, cl, 'o-', color='blue') \n",
    "plt.plot(iteration, co, 'o-', color='red') \n",
    "\n",
    "plt.ylim(0.865, 0.905) # adjust to fit as needed\n",
    "\n",
    "plt.title('Loss Weight Ratio Testing ROC AUC Results Zoomed')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('ROC AUC Value (One Vs. Rest Weighted for Class Output)')\n",
    "    \n",
    "red_patch = mpatches.Patch(color='red', label='Binary Output')\n",
    "blu_patch = mpatches.Patch(color='blue', label='Class Output')\n",
    "plt.legend(handles=[red_patch, blu_patch])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = LogisticRegression(solver='liblinear', random_state=0)\n",
    "baseline.fit(x_train, y_train_count)\n",
    "baseline_pred = baseline.predict_proba(x_test)\n",
    "baseline_pred = baseline_pred[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the random initial weightings of the following models mean that if 0.5 ROC AUC results are produced,\n",
    "# you should re-run them and they will work properly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Binary Model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model and checkpoints\n",
    "binary_model = build_multi_output_model(*optimal_parameters[:-1])\n",
    "\n",
    "compile_model(binary_model, optimal_parameters, 10, 0)\n",
    "\n",
    "binary_checkpoint = ModelCheckpoint(\"Binary_Checkpoint\", monitor='val_loss', verbose=1,\n",
    "    save_best_only=True, mode='min', save_freq='epoch')\n",
    "\n",
    "# Train model\n",
    "binary_model_history = binary_model.fit(x_train, [y_train_count, y_train_area], \n",
    "                    epochs=30,\n",
    "                    callbacks=[binary_checkpoint],\n",
    "                    verbose = 1,\n",
    "                    validation_split=0.2\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "best_binary_model = tensorflow.keras.models.load_model('Binary_Checkpoint')\n",
    "\n",
    "# Make predictions\n",
    "binary_y_pred=best_binary_model.predict(x_test)\n",
    "binary_count_predictions, binary_area_predictions = binary_y_pred[0], binary_y_pred[1]\n",
    "\n",
    "binary_auc = roc_auc_score(y_test_count, binary_count_predictions)\n",
    "baseline_auc = roc_auc_score(y_test_count, baseline_pred)\n",
    "print('Baseline:     ROC AUC=%.6f' % (baseline_auc))\n",
    "print('Binary Model: ROC AUC=%.6f' % (binary_auc))\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "ns_probs = [0 for _ in range(len(y_test_count))]\n",
    "ns1, ns2, _ = roc_curve(y_test_count, ns_probs)\n",
    "nn1, nn2, _ = roc_curve(y_test_count, binary_count_predictions)\n",
    "bl1, bl2, _ = roc_curve(y_test_count, baseline_pred)\n",
    "plt.plot(ns1, ns2, linestyle='--', label='No Skill')\n",
    "plt.plot(nn1, nn2, marker='.', color='blue',label='ROC curve for Binary Model ('+'%.5f' % (binary_auc)+')')\n",
    "plt.plot(bl1, bl2, marker='.', color='red', label='ROC curve for Baseline Model ('+'%.5f' % (baseline_auc)+')')\n",
    "\n",
    "# axis labels\n",
    "plt.title('ROC Curves for Binary Model and Baseline')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.plot(figsize=(6, 12))\n",
    "plt.show()\n",
    "\n",
    "best_epochs_b = np.argmin(binary_model_history.history['val_loss'])\n",
    "\n",
    "history = binary_model_history\n",
    "plt.subplot()\n",
    "plt.title('Binary Model Overall loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.axvline(x=best_epochs_b, label='Epoch with lowest val_loss = ' + '%.0f' % (best_epochs_b), color='green')\n",
    "plt.legend()\n",
    "# plt.savefig(\"binary_model_loss.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Class Model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model and checkpoints\n",
    "class_model  = build_multi_output_model(*optimal_parameters[:-1])\n",
    "\n",
    "compile_model(class_model, optimal_parameters, 8, 2)\n",
    "\n",
    "class_checkpoint = ModelCheckpoint(\"Class_Checkpoint\", monitor='val_loss', verbose=1,\n",
    "    save_best_only=True, mode='min', save_freq='epoch')\n",
    "\n",
    "# Train model\n",
    "class_model_history = class_model.fit(x_train, [y_train_count, y_train_area], \n",
    "                    epochs=30,\n",
    "                    callbacks=[class_checkpoint],\n",
    "                    verbose = 1,\n",
    "                    validation_split=0.2\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "best_class_model = tensorflow.keras.models.load_model('Class_Checkpoint')\n",
    "\n",
    "# Make predictions\n",
    "class_y_pred=best_class_model.predict(x_test)\n",
    "class_count_predictions, class_area_predictions = class_y_pred[0], class_y_pred[1]\n",
    "\n",
    "# Test Predictions\n",
    "macro_roc_auc_ovo = roc_auc_score(y_test_area, class_area_predictions, multi_class=\"ovo\",\n",
    "                                  average=\"macro\")\n",
    "weighted_roc_auc_ovo = roc_auc_score(y_test_area, class_area_predictions, multi_class=\"ovo\",\n",
    "                                     average=\"weighted\")\n",
    "macro_roc_auc_ovr = roc_auc_score(y_test_area, class_area_predictions, multi_class=\"ovr\",\n",
    "                                  average=\"macro\")\n",
    "weighted_roc_auc_ovr = roc_auc_score(y_test_area, class_area_predictions, multi_class=\"ovr\",\n",
    "                                     average=\"weighted\")\n",
    "print(\"One-vs-One ROC AUC scores:\\n{:.6f} (macro),\\n{:.6f} \"\n",
    "      \"(weighted by prevalence)\"\n",
    "      .format(macro_roc_auc_ovo, weighted_roc_auc_ovo))\n",
    "print(\"One-vs-Rest ROC AUC scores:\\n{:.6f} (macro),\\n{:.6f} \"\n",
    "      \"(weighted by prevalence)\"\n",
    "      .format(macro_roc_auc_ovr, weighted_roc_auc_ovr))\n",
    "\n",
    "multiclass_roc_auc = plot_multiclass_roc(y_test_area, class_area_predictions, n_classes=4, figsize=(6, 6))\n",
    "\n",
    "best_epoch = np.argmin(class_model_history.history['val_loss'])\n",
    "\n",
    "history = class_model_history\n",
    "plt.subplot()\n",
    "plt.title('Class Model Overall loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.axvline(x=best_epoch, label='Epoch with lowest val_loss = ' + '%.0f' % (best_epoch), color='green')\n",
    "plt.legend()\n",
    "# plt.savefig(\"class_model_loss.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Ablation Study</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_variable(data, variable1, variable2):\n",
    "\n",
    "    # Repeat preprocessing, but do so with one variable removed\n",
    "    data_y = data['Slide_Class']\n",
    "    data_x = data.drop(['SU_ID', 'Count', 'Area_Slide', 'Slide_Class'], axis=1)\n",
    "    if variable1 == variable2:\n",
    "        data_x = data_x.drop(variable1, axis=1)\n",
    "        data_x = data_x.astype(float)\n",
    "    else:\n",
    "        data_x = data_x.drop(variable1, axis=1)\n",
    "        data_x = data_x.drop(variable2, axis=1)\n",
    "        data_x = data_x.astype(float)\n",
    "\n",
    "    \n",
    "    x_train, x_test, y_train_area, y_test_area = train_test_split(data_x, \n",
    "                                                                  data_y, \n",
    "                                                                  test_size=0.2, \n",
    "                                                                  random_state=0)\n",
    "    \n",
    "    scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    train_x_values = x_train.values\n",
    "    x_scaled1 = scaler.fit_transform(train_x_values)\n",
    "    x_train = pd.DataFrame(x_scaled1)\n",
    "\n",
    "    test_x_values = x_test.values\n",
    "    x_scaled2 = scaler.fit_transform(test_x_values)\n",
    "    x_test = pd.DataFrame(x_scaled2)\n",
    "    \n",
    "    oversample = SMOTE()\n",
    "    x_train, y_train_area = oversample.fit_resample(x_train, y_train_area)\n",
    "    \n",
    "    encoder = LabelEncoder()\n",
    "\n",
    "    #training\n",
    "    encoder.fit(y_train_area)\n",
    "    encoded_Y = encoder.transform(y_train_area)\n",
    "    y_train = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "    #testing\n",
    "    encoder.fit(y_test_area)\n",
    "    encoded_Y_test = encoder.transform(y_test_area)\n",
    "    y_test = np_utils.to_categorical(encoded_Y_test)\n",
    "    \n",
    "    y_train_count = []\n",
    "    y_test_count = []\n",
    "\n",
    "    for i in y_train_area:\n",
    "        if i == \"Class 0\":\n",
    "            y_train_count.append(0)\n",
    "        else:\n",
    "            y_train_count.append(1)\n",
    "\n",
    "    for i in y_test_area:\n",
    "        if i == \"Class 0\":\n",
    "            y_test_count.append(0)\n",
    "        else:\n",
    "            y_test_count.append(1)\n",
    "\n",
    "    y_train_count = np.array(y_train_count)\n",
    "    y_test_count = np.array(y_test_count)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test_area, y_train_count, y_test_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanatory_variable_types = ['Area_SU', 'DIst', 'Slope', 'VRM', 'PRC', 'PLC', 'EAST', 'North', 'PGA', 'Relief']\n",
    "remove_com = []\n",
    "\n",
    "for i in explanatory_variable_types:\n",
    "    temp = []\n",
    "    for j in data.columns:\n",
    "        if i in j:\n",
    "            temp.append(j)\n",
    "    if temp != []:\n",
    "        if len(temp) == 1:\n",
    "            temp.append(temp[0])\n",
    "            remove_com.append(temp)\n",
    "        else:\n",
    "            remove_com.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation_results = []\n",
    "for j in tqdm(remove_com):\n",
    "    auc0 = []\n",
    "    auc1 = []\n",
    "    auc2 = []\n",
    "    auc3 = []\n",
    "    for itr in range(10):\n",
    "        ab_x_train, ab_x_test, ab_y_train, ab_y_test_area, ab_y_train_count, ab_y_test_count = remove_variable(data, *j)\n",
    "        \n",
    "        ab_class_model = build_multi_output_model(ab_x_train.shape[1],*optimal_parameters[1:-1])\n",
    "        \n",
    "        compile_model(ab_class_model, optimal_parameters, 8, 2)\n",
    "\n",
    "        ab_class_history = ab_class_model.fit(ab_x_train, [ab_y_train_count, ab_y_train], \n",
    "                            epochs=30,\n",
    "                            verbose = 0,\n",
    "                            validation_split=0.2\n",
    "                           )\n",
    "        class_y_pred=ab_class_model.predict(ab_x_test)\n",
    "        class_area_predictions = class_y_pred[1]\n",
    "        \n",
    "        # Compute ROC curve and ROC area for each class\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        \n",
    "                        \n",
    "        y_test_dummies = pd.get_dummies(ab_y_test_area, drop_first=False).values\n",
    "        for i in range(4):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_test_dummies[:, i], class_area_predictions[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "            if i == 0:\n",
    "                auc0.append(roc_auc[i])\n",
    "            if i == 1:\n",
    "                auc1.append(roc_auc[i])\n",
    "            if i == 2:\n",
    "                auc2.append(roc_auc[i])\n",
    "            if i == 3:\n",
    "                auc3.append(roc_auc[i])\n",
    "    result = [j, auc0, auc1, auc2, auc3]\n",
    "    ablation_results.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename variables for more clear understanding of the following cell\n",
    "area    = ablation_results[0][1:]\n",
    "dist    = ablation_results[1][1:]\n",
    "slope   = ablation_results[2][1:]\n",
    "vrm     = ablation_results[3][1:]\n",
    "prc     = ablation_results[4][1:]\n",
    "plc     = ablation_results[5][1:]\n",
    "east    = ablation_results[6][1:]\n",
    "north   = ablation_results[7][1:]\n",
    "pga     = ablation_results[8][1:]\n",
    "relief  = ablation_results[9][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 10))\n",
    "\n",
    "colors = ['green', 'orange', 'brown', 'red']\n",
    "green = dict(color=colors[0])\n",
    "orange = dict(color=colors[1])\n",
    "brown = dict(color=colors[2])\n",
    "red = dict(color=colors[3])\n",
    "\n",
    "ax.boxplot(area[0], positions=[1], boxprops=green, medianprops=green, whiskerprops=green, capprops=green)\n",
    "ax.boxplot(dist[0], positions=[2], boxprops=green, medianprops=green, whiskerprops=green, capprops=green)\n",
    "ax.boxplot(slope[0], positions=[3], boxprops=green, medianprops=green, whiskerprops=green, capprops=green)\n",
    "ax.boxplot(vrm[0], positions=[4], boxprops=green, medianprops=green, whiskerprops=green, capprops=green)\n",
    "ax.boxplot(prc[0], positions=[5], boxprops=green, medianprops=green, whiskerprops=green, capprops=green)\n",
    "ax.boxplot(plc[0], positions=[6], boxprops=green, medianprops=green, whiskerprops=green, capprops=green)\n",
    "ax.boxplot(east[0], positions=[7], boxprops=green, medianprops=green, whiskerprops=green, capprops=green)\n",
    "ax.boxplot(north[0], positions=[8], boxprops=green, medianprops=green, whiskerprops=green, capprops=green)\n",
    "ax.boxplot(pga[0], positions=[9], boxprops=green, medianprops=green, whiskerprops=green, capprops=green)\n",
    "ax.boxplot(relief[0], positions=[10], boxprops=green, medianprops=green, whiskerprops=green, capprops=green)\n",
    "\n",
    "ax.boxplot(area[1], positions=[11], boxprops=orange, medianprops=orange, whiskerprops=orange, capprops=orange)\n",
    "ax.boxplot(dist[1], positions=[12], boxprops=orange, medianprops=orange, whiskerprops=orange, capprops=orange)\n",
    "ax.boxplot(slope[1], positions=[13], boxprops=orange, medianprops=orange, whiskerprops=orange, capprops=orange)\n",
    "ax.boxplot(vrm[1], positions=[14], boxprops=orange, medianprops=orange, whiskerprops=orange, capprops=orange)\n",
    "ax.boxplot(prc[1], positions=[15], boxprops=orange, medianprops=orange, whiskerprops=orange, capprops=orange)\n",
    "ax.boxplot(plc[1], positions=[16], boxprops=orange, medianprops=orange, whiskerprops=orange, capprops=orange)\n",
    "ax.boxplot(east[1], positions=[17], boxprops=orange, medianprops=orange, whiskerprops=orange, capprops=orange)\n",
    "ax.boxplot(north[1], positions=[18], boxprops=orange, medianprops=orange, whiskerprops=orange, capprops=orange)\n",
    "ax.boxplot(pga[1], positions=[19], boxprops=orange, medianprops=orange, whiskerprops=orange, capprops=orange)\n",
    "ax.boxplot(relief[1], positions=[20], boxprops=orange, medianprops=orange, whiskerprops=orange, capprops=orange)\n",
    "\n",
    "ax.boxplot(area[2], positions=[21], boxprops=brown, medianprops=brown, whiskerprops=brown, capprops=brown)\n",
    "ax.boxplot(dist[2], positions=[22], boxprops=brown, medianprops=brown, whiskerprops=brown, capprops=brown)\n",
    "ax.boxplot(slope[2], positions=[23], boxprops=brown, medianprops=brown, whiskerprops=brown, capprops=brown)\n",
    "ax.boxplot(vrm[2], positions=[24], boxprops=brown, medianprops=brown, whiskerprops=brown, capprops=brown)\n",
    "ax.boxplot(prc[2], positions=[25], boxprops=brown, medianprops=brown, whiskerprops=brown, capprops=brown)\n",
    "ax.boxplot(plc[2], positions=[26], boxprops=brown, medianprops=brown, whiskerprops=brown, capprops=brown)\n",
    "ax.boxplot(east[2], positions=[27], boxprops=brown, medianprops=brown, whiskerprops=brown, capprops=brown)\n",
    "ax.boxplot(north[2], positions=[28], boxprops=brown, medianprops=brown, whiskerprops=brown, capprops=brown)\n",
    "ax.boxplot(pga[2], positions=[29], boxprops=brown, medianprops=brown, whiskerprops=brown, capprops=brown)\n",
    "ax.boxplot(relief[2], positions=[30], boxprops=brown, medianprops=brown, whiskerprops=brown, capprops=brown)\n",
    "\n",
    "ax.boxplot(area[3], positions=[31], boxprops=red, medianprops=red, whiskerprops=red, capprops=red)\n",
    "ax.boxplot(dist[3], positions=[32], boxprops=red, medianprops=red, whiskerprops=red, capprops=red)\n",
    "ax.boxplot(slope[3], positions=[33], boxprops=red, medianprops=red, whiskerprops=red, capprops=red)\n",
    "ax.boxplot(vrm[3], positions=[34], boxprops=red, medianprops=red, whiskerprops=red, capprops=red)\n",
    "ax.boxplot(prc[3], positions=[35], boxprops=red, medianprops=red, whiskerprops=red, capprops=red)\n",
    "ax.boxplot(plc[3], positions=[36], boxprops=red, medianprops=red, whiskerprops=red, capprops=red)\n",
    "ax.boxplot(east[3], positions=[37], boxprops=red, medianprops=red, whiskerprops=red, capprops=red)\n",
    "ax.boxplot(north[3], positions=[38], boxprops=red, medianprops=red, whiskerprops=red, capprops=red)\n",
    "ax.boxplot(pga[3], positions=[39], boxprops=red, medianprops=red, whiskerprops=red, capprops=red)\n",
    "ax.boxplot(relief[3], positions=[40], boxprops=red, medianprops=red, whiskerprops=red, capprops=red)\n",
    "\n",
    "ax.axhline(y=multiclass_roc_auc[0], xmin=0, xmax=0.25, color='green')\n",
    "ax.axhline(y=multiclass_roc_auc[1], xmin=0.25, xmax=0.5, color='orange')\n",
    "ax.axhline(y=multiclass_roc_auc[2], xmin=0.5, xmax=0.75, color='brown')\n",
    "ax.axhline(y=multiclass_roc_auc[3], xmin=0.75, xmax=1, color='red')\n",
    "\n",
    "green_patch = mpatches.Patch(color='green', label='Class 0 Ablation Results With Class Model ROC AUC Reference')\n",
    "orange_patch = mpatches.Patch(color='orange', label='Class 1 Ablation Results With Class Model ROC AUC Reference')\n",
    "brown_patch = mpatches.Patch(color='brown', label='Class 2 Ablation Results With Class Model ROC AUC Reference')\n",
    "red_patch = mpatches.Patch(color='red', label='Class 3 Ablation Results With Class Model ROC AUC Reference')\n",
    "\n",
    "ax.legend(handles=[green_patch, orange_patch, brown_patch, red_patch])\n",
    "\n",
    "plt.xticks([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40], \n",
    "           ['Area_SU', 'Dist2Str', 'Slope', 'VRM', 'PRC', 'PLC', 'Eastness', 'Northness', 'PGA', 'Relief', 'Area_SU', 'Dist2Str', 'Slope', 'VRM', 'PRC', 'PLC', 'Eastness', 'Northness', 'PGA', 'Relief', 'Area_SU', 'Dist2Str', 'Slope', 'VRM', 'PRC', 'PLC', 'Eastness', 'Northness', 'PGA', 'Relief', 'Area_SU', 'Dist2Str', 'Slope', 'VRM', 'PRC', 'PLC', 'Eastness', 'Northness', 'PGA', 'Relief'],\n",
    "          rotation='vertical')\n",
    "\n",
    "plt.title('Ablation Study Results, Class Breakdown')\n",
    "plt.xlabel('Variable')\n",
    "plt.ylabel('ROC AUC')\n",
    "plt.savefig(\"class_ablation.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_ablation_results = []\n",
    "\n",
    "for i in tqdm(remove_com):\n",
    "    vals = []\n",
    "    for itr in range(10):\n",
    "        ab_x_train, ab_x_test, ab_y_train, ab_y_test_area, ab_y_train_count, ab_y_test_count = remove_variable(data, *i)\n",
    "\n",
    "        ab_binary_model = build_multi_output_model(ab_x_train.shape[1],*optimal_parameters[1:-1])\n",
    "        \n",
    "        compile_model(ab_binary_model, optimal_parameters, 10, 0)\n",
    "        \n",
    "        bin_history = ab_binary_model.fit(ab_x_train, [ab_y_train_count, ab_y_train], \n",
    "                            epochs=30,\n",
    "                            verbose = 0,\n",
    "                            validation_split=0.2\n",
    "                           )\n",
    "        binary_y_pred=ab_binary_model.predict(ab_x_test)\n",
    "        binary_predictions = binary_y_pred[0]\n",
    "        binary_auc = roc_auc_score(ab_y_test_count, binary_predictions)   \n",
    "        vals.append(binary_auc)\n",
    "        \n",
    "    result = [i, vals]\n",
    "    bin_ablation_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "colors = ['green', 'orange', 'brown', 'red']\n",
    "green = dict(color=colors[0])\n",
    "orange = dict(color=colors[1])\n",
    "brown = dict(color=colors[2])\n",
    "red = dict(color=colors[3])\n",
    "\n",
    "\n",
    "plt.boxplot(bin_ablation_results[0][1], positions=[1])\n",
    "plt.boxplot(bin_ablation_results[1][1], positions=[2])\n",
    "plt.boxplot(bin_ablation_results[2][1], positions=[3])\n",
    "plt.boxplot(bin_ablation_results[3][1], positions=[4])\n",
    "plt.boxplot(bin_ablation_results[4][1], positions=[5])\n",
    "plt.boxplot(bin_ablation_results[5][1], positions=[6])\n",
    "plt.boxplot(bin_ablation_results[6][1], positions=[7])\n",
    "plt.boxplot(bin_ablation_results[7][1], positions=[8])\n",
    "plt.boxplot(bin_ablation_results[8][1], positions=[9])\n",
    "plt.boxplot(bin_ablation_results[9][1], positions=[10])\n",
    "\n",
    "plt.axhline(y=binary_auc, color='green', label='OVR Weighted ROC AUC Average of Binary Model')\n",
    "plt.legend()\n",
    "\n",
    "plt.xticks([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \n",
    "           ['Area_SU', 'Dist2Str', 'Slope', 'VRM', 'PRC', 'PLC', 'Eastness', 'Northness', 'PGA', 'Relief'])\n",
    "\n",
    "plt.title('Ablation Study Results, Binary Model')\n",
    "plt.xlabel('Variable')\n",
    "plt.ylabel('ROC AUC')\n",
    "# plt.savefig(\"binary_ablation.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
